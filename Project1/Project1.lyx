#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass extarticle
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement ph
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Analysis on Methods: Root Finding and Interpolation
\end_layout

\begin_layout Author
Seth Shelnutt
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Abstract
Numerical Analysis, it is defined as 
\begin_inset Quotes eld
\end_inset

The branch of mathematics that deals with the development and use of numerical
 methods for solving problems
\begin_inset Quotes erd
\end_inset

.
 Since before the Greeks man has often sought to put some physical process
 or data into a mathematical representation.
 Since the shift into the modern atomic era this field of mathematics has
 become ever more important as computers are able to perform these numerical
 methods at levels never before seen.
 Two topics of this field are root finding and interpolation.
 Various methods of these topics will be examined and tested for accuracy
 and speed, with a focus on proper statistical analysis.
 
\end_layout

\begin_layout Abstract
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Abstract
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Paragraph*
When implementing the field of numerical analysis there are several factors
 to first consider.
 The very first question to answer is will this be done by hand or on a
 computer? This has a large impact on what methods can be considered to
 evaluate the problems.
 In this case a computer will be used.
 This thing brings up the question of what language to use for this analysis.
 There are three main languages which are considered standard.
 First for cases which require the most optimization and speed C is used.
 For a easy to use high level 
\begin_inset Quotes eld
\end_inset

language
\begin_inset Quotes erd
\end_inset

 matlab is often chosen.
 The problem with matlab is two fold, first it is commercial and only available
 on certain platforms.
 Second is that it's less of a language and more of an interactive terminal,
 thus making complex programming difficult.
 
\end_layout

\begin_layout Paragraph*
In the middle ground between these two languages lies Python.
 Python offers the robust mathematical support of matlab through numeric
 python (numpy), scientific python (scipy) and symbolic python (sympy).
 With the extension of Matplotlib, python also offers a diverse graphical
 package.
 Python is a higher level language than C, and offers many features such
 as garbage collecting and memory allocation.
 Code is run inside an interpreter which means it can be run on any platform
 python is support.
 By the open source nature of python, nearly any existing platform, be it
 mainstream or embedded is supported.
 Python also offers various methods of optimization, be it in the code,
 or the interpreter.
 Python also offers support for inline C code if needed.
 Overall python's rugged and robust offerings make it the ideal language
 for numerical analysis and thus will be used in this paper.
\end_layout

\begin_layout Section
Root Finding
\end_layout

\begin_layout Paragraph*
For the purpose of this section the object is to find the intersection of
 
\begin_inset Formula $e^{\frac{-x}{5}}$
\end_inset

 and 
\begin_inset Formula $\sin x$
\end_inset

.
 That is the roots of 
\begin_inset Formula $e^{\frac{-x}{5}}=\sin x$
\end_inset

 .
\end_layout

\begin_layout Subsection
Visual Inspection
\end_layout

\begin_layout Paragraph*
The first and simplest method for root finding is a visual inspection.
 With a visual inspection a graph is shown and a user selects an x value
 that is approximately a zero.
 Here matplotlib is used to create the graph of the two functions and the
 user is asked to click on the point in which the zero is approximated.
 From this the x value is given and the difference in 
\begin_inset Formula $e^{\frac{-x}{5}}$
\end_inset

 - 
\begin_inset Formula $\sin x$
\end_inset

.
 See Figure 1.
 
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement r
overhang 0in
width "50text%"
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename Visual.png
	display false
	scale 40

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Here the first of the four intersections is estimated to be, 
\begin_inset Formula $x=0.967742$
\end_inset

 and the difference between the two functions is: -0.000422834187542 .
 So depending on the interval and scale one can guess reasonably close,
 however the chances of visually selecting the intersection with a high
 degree of precision is unlikely.
 The remaining estimates on the intersections are at 
\begin_inset Formula $x=\left\{ 2.479839,6.572581,9.274194\right\} $
\end_inset

and the differences between the functions were 0.00552022494328 , 0.016768497586
 and -0.00646222207318.
 By and large in most cases a higher degree of accuracy is sought after.
 Thus a more mathematical process such as bisection or Newton's method is
 to be examined.
\end_layout

\begin_layout Subsection
Bisection
\end_layout

\begin_layout Paragraph
The bisection method stems from bisecting an interval in each iteration.
 A function and an interval is given.
 The interval is then bisected and a sign change is looked for on either
 side of midpoint.
 On the side that there is a sign change a new interval is formed between
 that end point and the midpoint.
 This then forms a new interval in which the next iteration takes place
 on.
 Mathematically we are doing: Given a bound 
\begin_inset Formula $\left[x_{1},x_{2}\right]$
\end_inset

, 
\begin_inset Formula $x_{3}=\frac{x_{1}+x_{2}}{2}$
\end_inset

, 
\begin_inset Formula $f(x_{1})f(x_{3})\begin{cases}
<0 & x_{2}=x_{3}\\
>0 & x_{1}=x_{3}\\
=0 & f(x_{3})=0
\end{cases}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Now that a formula is established there are two lingering questions.
 What is the convergence rate and what is error in this method? The maximum
 error is given by 
\begin_inset Formula $\frac{\Vert b-a\Vert}{2^{i}}$
\end_inset

, where i is the number of iterations run.
 This yields the convergence rate of 
\begin_inset Formula $\frac{b-a}{2^{i}}$
\end_inset

, or simply 
\begin_inset Formula $\frac{1}{2}$
\end_inset

.
 This resultant linear convergence is good, but when finding the zero of
 
\begin_inset Formula $e^{\frac{-x}{5}}=\sin x$
\end_inset

 , between 0 and 2 with python it takes 21 iterations and 0.000659 seconds.
 The zero is estimated by 
\begin_inset Formula $x=0.968319892883$
\end_inset

 While in this small instance that does not seem like a significant amount,
 on larger and more complex systems this could end up taking significant
 time and number of iterations.
 A quadratic approach was developed by Newton to quicken root finding and
 will be discussed in the next section.
\end_layout

\begin_layout Subsection
Newton's Method
\end_layout

\begin_layout Paragraph
Newton first developed his method for root finding in De analysi per aequationes
 numero terminorum infinitas written in 1669 and in De metodis fluxionum
 et serierum infinitarum written in 1671.
 Both of these description vary from the modern Newton's method as Newton
 failed to make the connection to Calculus and instead is based on a pure
 algebraic implementation.
 The modern interpretation of Newton's method and the one implemented in
 this project is 
\begin_inset Formula $x_{n+1}=x_{n}+\frac{f(x_{n})}{f'(x_{n})}$
\end_inset

 where you test for 
\begin_inset Formula $f(x_{n+1})=0$
\end_inset

.
 Here as with the bisection the function the zero is being determined is
 
\begin_inset Formula $e^{\frac{-x}{5}}-\sin x=0$
\end_inset

.
 The convergence of Newton's method is what gives it an advantage over the
 bisection method.
 The convergence is given by 
\begin_inset Formula $lim_{n\rightarrow\text{âˆž}}\frac{x_{n+1}-\alpha}{(x_{n}-\alpha)^{2}}=\frac{f^{2}(a)}{2}$
\end_inset

.
 Thus Newton's method yield quadratic convergence on the root.
\end_layout

\begin_layout Paragraph
When implementing Newton's method in python intriguing results are found.
 Starting at the same initial point as in bisection, 
\begin_inset Formula $x_{1}=0$
\end_inset

, it takes only 4 iterations to find the zero at 
\begin_inset Formula $x=0.968319798371$
\end_inset

.
 How it takes 0.1072 seconds to calculate this.
 This is an astonishing 162 times longer than the bisection method.
 Even though the number of iteration was decreased by over 80% the time
 it takes to calculate the derivative yields in a more timely process.
 The the optimization section a more detailed analysis of this time differential
 and methods to improve speed will be examined.
\end_layout

\begin_layout Section
Interpolation
\end_layout

\begin_layout Paragraph
Interpolation is another of the most commonly used applications of numerical
 analysis.
 Interpolation is defined as the construction of a new set of data points,
 or equation, within a defined ranged give known data points.
 There are several methods and techniques for interpolation ranging in accuracy,
 speed, and power.
 The function that is being interpolated in this case is 
\begin_inset Formula $f(x)=\frac{1}{1+x^{2}}$
\end_inset

 over the range 
\begin_inset Formula $\left[-5,5\right]$
\end_inset

.
\end_layout

\begin_layout Subsection
Lagrange Polynomials
\end_layout

\begin_layout Paragraph
Lagrange polynomials were first published by Waring in 1779, only to be
 rediscovered by Euler in 1783, and published by Lagrange in 1795.
 Lagrange polynomials are unique and given by 
\begin_inset Formula $P_{n}(x)=\sum\limits _{i=0}^{n}f(x_{i})L_{n}(x)$
\end_inset

, where 
\begin_inset Formula $L_{n}(x)=\prod\limits _{j=k,j=0}^{n}\frac{x-x_{k}}{x_{j}-x_{k}}$
\end_inset

.
\end_layout

\begin_layout Paragraph
When constructing interpolating polynomials, there is an inherit problem
 with having a better fit and having a smooth fitting function.
 The more data points in the interpolation, the higher the degree of the
 resulting polynomial, and therefore the greater oscillation it will exhibit
 between the data points.
 At the same time the accuracy at the data points will be very high.
 
\end_layout

\begin_layout Paragraph
As always when estimate is used, the error value is important.
 How good and how close is our estimations? The error can be calculated
 by 
\begin_inset Formula $f(x)-P_{n}(x)=\prod\limits _{x=0}^{n}\frac{x-x_{n}}{(n+1)!}f^{n+1}(\xi)$
\end_inset

.
\end_layout

\begin_layout Paragraph
For the function 
\begin_inset Formula $f(x)=\frac{1}{1+x^{2}}$
\end_inset

 over the range 
\begin_inset Formula $\left[-5,5\right]$
\end_inset

, three separate Lagrange polynomials will be examined.
 First with a degree of n=5, then n=10 and lastly n=20.
 With each of these n+1 points will be selected on an even distribution
 between 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\left[-5,5\right]$
\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 See figure 2 for a graph showing all three polynomials.
\end_layout

\begin_layout Paragraph
When n = 5, the number of data points present is six.
 Using evenly space points, it takes 0.3169 seconds to calculate the polynomial
 of 
\begin_inset Formula $P_{5}=0.0019231x^{4}-0.069231x^{2}+0.567308$
\end_inset

.
 When n=10, it take 0.9429 seconds to calculate the polynomial of 
\begin_inset Formula $P_{10}=-.000023x^{10}-1.694066e^{^{-5}}x^{9}+0.001267x^{8}-0.024412x^{6}-4.33681e^{-19}x^{5}+0.197376x^{4}+2.151057e^{-16}x^{3}-0.674208x^{2}+1.994932e^{-17}x+1.0.$
\end_inset

A Lagrange polynomial of order 20 it takes over 5.1059 seconds to calculate.
 The resulting polynomial is 
\begin_inset Formula $\ensuremath{P_{20}=2.72817e^{-19}x^{20}-2.65314e^{-7}x^{18}-4.23516e^{-22}x^{17}+1.07425e^{-5}x^{16}-4.74338e^{-20}x^{15}-0.000236x^{14}+7.21862e^{-16}x^{13}+0.003102x^{12}+5.57367e^{-15}x^{11}-0.025114x^{10}+1.40582e^{-14}x^{9}+0.126253x^{8}-7.21645e^{-16}x^{7}-0.391630x^{6}-3.33067e^{-16}x^{5}+0.753354x^{4}+2.94209e^{-15}x^{3}-0.965739x^{2}-8.32667e^{-17}x+1.0}.$
\end_inset

 There are two main things to be taken from these three Lagrange polynomials.
 First is that as expected a higher degree polynomial leads to a much better
 fit at the data points.
 However as can be seen from the graph at the end points the variation becomes
 extreme.
 Second the time taken to calculate the nth degree polynomial is exponential.
 To calculate a 10th degree polynomial takes just under 1 seconds, but a
 20th degree takes over 5 seconds.
 The usefulness of Lagrange polynomials comes into question with larger
 values of n.
 Later in the Optimization section methods to improve on the speed of Lagrange
 interpolation will be examined.
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement o
overhang 0col%
width "50col%"
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename Lagrange-5-10-20.png
	display false
	scale 42

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Piecewise Interpolation
\end_layout

\begin_layout Paragraph
Linear or Piecewise Interpolation is the simplest form of interpolation.
 It is linear and does not create a polynomial.
 Instead the data points are simply 
\begin_inset Quotes eld
\end_inset

connected
\begin_inset Quotes erd
\end_inset

.
 It has the benefit of being a simple linear calculation between only two
 points.
 Speed there for is very high and accuracy can be increased by using additional
 points.
 Unlike Lagrange polynomials there is no significant disadvantages to more
 points, such as increased variation.
 The main issue with linear interpolation such as this, is in the case of
 data which is a polynomial such as here, it simply doesn't provide a good
 fit.
 Thus a larger data set is essential for a more accurate fit.
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement r
overhang 0in
width "50col%"
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename Linear-5-10-20.png
	display false
	scale 45

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
As with Lagrange once again the interpolation is 
\begin_inset Formula $f(x)=\frac{1}{1+x^{2}}$
\end_inset

 over the range 
\begin_inset Formula $\left[-5,5\right]$
\end_inset

, with 5, 10 and 20 data points.
 See figure 3 for a graph of all three functions.
 The time taken to calculate n=5 was 0.00224 seconds, while n=10 was 0.00835
 seconds, and n=20 was 0.01703 seconds.
 Unsurprisingly this was significantly faster than Lagrange interpolation.
 Below is the piecewise functions for these three interpolations.
\end_layout

\begin_layout Paragraph
When n = 5, 
\begin_inset Formula $I_{5}(x)=\begin{cases}
0.030769x+0.115385 & \; for\,-5<x<-3\\
0.2x+0.5 & \; for\,-3<x<-1\\
-0.5 & \; for\,-1<x<1\\
-0.2x-0.3 & \; for\,1<x<3\\
-0.030769x-0.007692 & \; for\,3<x<5
\end{cases}$
\end_inset


\end_layout

\begin_layout Paragraph
When n = 10, 
\begin_inset Formula $I_{10}(x)=\begin{cases}
0.020362x+0.063349 & \; for\,-5<x<-4\\
0.041176x+0.105882 & \; for\,-4<x<-3\\
0.1x+0.2 & \; for\,-3<x<-2\\
0.3x+0.4 & \; for\,-2<x<-1\\
0.5x & \; for\,-1<x<0\\
-0.5x-1 & \; for\,0<x<1\\
-0.3x-0.2 & \; for\,1<x<2\\
-0.1x & \; for\,2<x<3\\
-0.041176x+0.023529 & \; for\,3<x<4\\
-0.020362x+0.022624 & \; for\,4<x<5
\end{cases}$
\end_inset


\end_layout

\begin_layout Paragraph
When n = 20, 
\begin_inset Formula $I_{20}(x)=\begin{cases}
0.017195x+0.047511 & \; for\,-5<x<-4.5\\
0.023529x+0.058824 & \; for\,-4.5<x<-4\\
0.033296x+0.074362 & \; for\,-4<x<-3.5\\
0.049056x+0.096226 & \; for\,-3.5<x<-3\\
0.075862x+0.127586 & \; for\,-3<x<-2.5\\
0.124138x+0.172414 & \; for\,-2.5<x<-2\\
0.215385x+0.230769 & \; for\,-2<x<-1.5\\
0.384615x+0.269231 & \; for\,-1.5<x<-1\\
0.6x+.01 & \; for\,-1<x<-0.5\\
0.4x-0.6 & \; for\,-0.5<x<0\\
-0.4x-1 & \; for\,0<x<0.5\\
-0.6x-0.5 & \; for\,0.5<x<1\\
-0.384615x-0.115385 & \; for\,1<x<1.5\\
-0.215385x+0.015385 & \; for\,1.5<x<2\\
-0.124138x+0.048276 & \; for\,2<x<2.5\\
-0.075862x+0.051724 & \; for\,2.5<x<3\\
-0.490566x+0.047170 & \; for\,3<x<3.5\\
-0.033296x+0.041065 & \; for\,3.5<x<4\\
-0.023529x+0.035294 & \; for\,4<x<4.5\\
-0.017195x+0.030317 & \; for\,4.5<x<5
\end{cases}$
\end_inset


\end_layout

\begin_layout Subsection
Raised Cosine Interpolation
\end_layout

\begin_layout Paragraph
Raised cosine interpolation stems from attempting to find a better fit than
 linear interpolation and Lagrange.
 Using cosine provides a 
\begin_inset Quotes eld
\end_inset

nicer
\begin_inset Quotes erd
\end_inset

, that is smoother, fit between points than linear and higher order Lagrange.
 Since cosine is a continuously smooth curve, interpolating between points
 yields a continuous function.
\end_layout

\end_body
\end_document
